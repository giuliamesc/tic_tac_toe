{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package loading\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlayer:\n",
    "    \"Class for a Q-Learning agent player\"\n",
    "    states_value = dict() # q_value table, shared by all class instances \n",
    "    def __init__(self, epsilon, alpha = 0.05, gamma = 0.99, player_name = 'O'):\n",
    "        \"\"\"\n",
    "        Initializer.\n",
    "        INPUTS: \n",
    "        - epsilon: exploration level to define the epsilon-greedy policy; the agent takes the best action with probability 1-epsilon\n",
    "        - alpha: learning rate\n",
    "        - gamma: discount factor\n",
    "        - player_name: player symbol on the board\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha \n",
    "        self.gamma = gamma \n",
    "        self.player_name = player_name \n",
    "\n",
    "    def set_epsilon(self, epsilon):\n",
    "        \"\"\"\n",
    "        Functions that sets the exploration level.\n",
    "        INPUTS:\n",
    "        - epsilon: exploration level\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def set_player(self, player_name = 'O', j=-1):\n",
    "        \"\"\"\n",
    "        Function to set player symbol, needed to alternate the starting player.\n",
    "        INPUTS:\n",
    "        - player_name: player symbol on the board\n",
    "        - j : number of the current game\n",
    "        \"\"\"\n",
    "        self.player_name = player_name\n",
    "        if j != -1:\n",
    "            self.player_name = 'O' if j % 2 == 0 else 'X'\n",
    "            \n",
    "    def empty(self, grid):\n",
    "        \"\"\"\n",
    "        Function to return available actions given a board situation.\n",
    "        INPUTS: \n",
    "        - grid: current board status\n",
    "        OUTPUTS:\n",
    "        - avail: array with available actions\n",
    "        \"\"\"\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(i)\n",
    "        return avail\n",
    "    \n",
    "    def get_state_key(self, grid):\n",
    "        \"\"\"\n",
    "        Function transforing the grid configuration into a string, needed for use it as key of a dictionary.\n",
    "        INPUTS:\n",
    "        - grid: a board status\n",
    "        OUTPUTS:\n",
    "        - key: correspondent string\n",
    "        \"\"\" \n",
    "        key = str(grid.reshape(3 * 3))\n",
    "        return key\n",
    "\n",
    "    def correct_grid(self, grid):\n",
    "        \"\"\"\n",
    "        Function for correct storage of states: boards with same configurations for the q-player, but with different chesses should count as the same state.\n",
    "        INPUTS:\n",
    "        - grid: a board status\n",
    "        OUTPUT:\n",
    "        - a string associated to the state, with the convention that the q-player positions are always represented by \"1\"\n",
    "        \"\"\"\n",
    "        if self.player_name == 'X': # we choose to store states with \"1.\" <-> \"X\" to indicate the q_player\n",
    "            return self.get_state_key(grid)\n",
    "        else:\n",
    "            new_grid = -grid\n",
    "            return self.get_state_key(new_grid)\n",
    "    \n",
    "    def select_optimal_action(self, grid):\n",
    "        \"\"\"\n",
    "        Function selecting the available action with the highest Q-value, given a state.\n",
    "        INPUTS:\n",
    "        - grid: a board status\n",
    "        OUTPUTS:\n",
    "        - move: action with the highest Q-value\n",
    "        \"\"\"\n",
    "        avail_actions = self.empty(grid) # find all available actions\n",
    "        key = self.correct_grid(grid) # get the string corresponding to the current state\n",
    "        if key in self.states_value : \n",
    "            # IF THE STATE HAS ALREADY BEEN EXPLORED\n",
    "            restricted_vector = self.states_value[key][avail_actions] # array with the Q-values of the available actions\n",
    "            idx = np.argwhere(restricted_vector == np.amax(restricted_vector))[:,0] # position of maximum of restricted_vector\n",
    "            move = avail_actions[random.choice(idx)] # pick randomply one of the actions with maximum value\n",
    "        else :\n",
    "            # IF THE STATE IS EXPLORED FOR THE FIRST TIME\n",
    "            move = self.select_random_action(grid) # pick randomly the action to do \n",
    "        return move\n",
    "    \n",
    "    def select_random_action(self,grid):\n",
    "        \"\"\"\n",
    "        Function selecting a random available action, given a state.\n",
    "        INPUTS:\n",
    "        - grid: a board status\n",
    "        OUTPUTS:\n",
    "        - move: a random available action\n",
    "        \"\"\"\n",
    "        key = self.correct_grid(grid)\n",
    "        # IF THE STATE IS EXPLORED FOR THE FIRST TIME\n",
    "        if key not in self.states_value:\n",
    "                self.states_value[key] = np.zeros([9,1]) # insert the key into the dictionary and a zero-array for the values\n",
    "        actions = self.empty(grid) # find all available actions\n",
    "        move = random.choice(actions) # pick randomly the action to do\n",
    "        return move\n",
    "    \n",
    "    def act(self,grid):\n",
    "        \"\"\"\n",
    "        Function selecting an available action given a state following the epsilon-greedy policy.\n",
    "        INPUTS:\n",
    "        - grid: a board status\n",
    "        OUTPUTS:\n",
    "        - move: an available action\n",
    "        \"\"\"\n",
    "        b = np.random.binomial(1,1-self.epsilon) # generation of a bernoulli random variable\n",
    "        # choose the action with the highest Q-value with probability 1-epsilon\n",
    "        if b == 1 : \n",
    "            return self.select_optimal_action(grid)\n",
    "            \n",
    "        else : \n",
    "            return self.select_random_action(grid)    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearning:\n",
    "    \"Class for learning from experts\"\n",
    "    def __init__(self, epsilon_q, epsilon_opp, alpha = 0.05, gamma = 0.99):\n",
    "        \"\"\"\n",
    "        Initializer\n",
    "        INPUTS:\n",
    "        - epsilon1: exploration level of q_player\n",
    "        - epsilon2: exploration level of opponent\n",
    "        - alpha: learning rate\n",
    "        - gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.q_player = QPlayer(epsilon_q, alpha, gamma) # Q-player\n",
    "        self.opponent = OptimalPlayer(epsilon_opp) # Opponent\n",
    "        self.env = TictactoeEnv() # Tic-Tac-Toe environment\n",
    "        self.reward_vec = [] # vector to store the rewards during several games\n",
    "\n",
    "        # thresholds for computing training time\n",
    "        self.M_rand_threshold = 0.856 * 0.8\n",
    "        self.M_opt_threshold = 0\n",
    "\n",
    "    def game_train(self, eps_q_player, eps_opponent = 0.5):\n",
    "        \"\"\"\n",
    "        Function to play a training game.\n",
    "        INPUTS:\n",
    "        - eps_q_player: exploration level of q_player\n",
    "        - eps_opponent: exploration level of opponent (default = 0.5)\n",
    "        \"\"\"\n",
    "        self.env.reset() # reset the board\n",
    "        \n",
    "        #setting exploration levels\n",
    "        self.q_player.epsilon = eps_q_player\n",
    "        self.opponent.epsilon = eps_opponent\n",
    "        # vector to store states, initialized with empty board\n",
    "        states = [] \n",
    "        states.append(self.env.grid.copy())\n",
    "        move_q = -1 # initialization of move of q_player\n",
    "        \n",
    "        while not self.env.end: # while the game is not over\n",
    "            \n",
    "            if self.env.current_player == self.q_player.player_name :\n",
    "                # action of the q_player\n",
    "                move_q = self.q_player.act(states[-1]) \n",
    "                self.env.step(move_q)\n",
    "            else:\n",
    "                # action of the opponent\n",
    "                move_opp = self.opponent.act(states[-1])\n",
    "                self.env.step(move_opp)\n",
    "            # storage of the grid after the move    \n",
    "            states.append(self.env.grid.copy())\n",
    "\n",
    "            # update the q_values before the move of the q_player, after the first move\n",
    "            if self.env.current_player == self.q_player.player_name and move_q != -1:\n",
    "                self.update_q(self.q_player.player_name, states[-3], states[-1], move_q)\n",
    "\n",
    "        # to avoid missing one case of update, when the game ends after the move of the q-player\n",
    "        if self.env.end and self.env.current_player == self.opponent.player :\n",
    "            self.update_q(self.q_player.player_name, states[-2], states[-1], move_q)\n",
    "\n",
    "        # return reward of the q-player\n",
    "        return self.env.reward(self.q_player.player_name)\n",
    "\n",
    "    def game_test(self, eps_q_player, eps_opponent = 0):\n",
    "        \"\"\"\n",
    "        Function to play a test game.\n",
    "        INPUTS:\n",
    "        - eps_q_player: exploration level of q_player\n",
    "        - eps_opponent: exploration level of opponent (default = 0.5)\n",
    "        \"\"\"\n",
    "        self.env.reset() # reset the board\n",
    "\n",
    "        #setting exploration levels\n",
    "        self.q_player.epsilon = eps_q_player\n",
    "        self.opponent.epsilon = eps_opponent\n",
    "        \n",
    "        while not self.env.end: # while the game is not over\n",
    "            \n",
    "            if self.env.current_player == self.q_player.player_name :\n",
    "                # action of the q_player\n",
    "                move_q = self.q_player.act(self.env.grid)\n",
    "                self.env.step(move_q)\n",
    "            else:\n",
    "                # action of the opponent\n",
    "                move_opp = self.opponent.act(self.env.grid)\n",
    "                self.env.step(move_opp)\n",
    "\n",
    "        # return reward of the q-player\n",
    "        return self.env.reward(self.q_player.player_name)\n",
    "         \n",
    "        \n",
    "    def update_q(self, player_name, state_prec, state, move_q):\n",
    "        \"\"\"\n",
    "        Function updating the table of Q-values.\n",
    "        INPUT:\n",
    "        - player_name: symbol of the player\n",
    "        - state_prec: precendent state visited by the agent\n",
    "        - state: state visited by the agent\n",
    "        - move_q: action performed by the agent to pass from state_prec to state\n",
    "        \"\"\"\n",
    "        r = self.env.reward(player_name) # get the reward of the q_player \n",
    "        # get strings correspondent to states\n",
    "        key = self.q_player.correct_grid(state)\n",
    "        key_prec = self.q_player.correct_grid(state_prec)\n",
    "        # check if the state is already present in states_value\n",
    "        if key not in self.q_player.states_value:\n",
    "            self.q_player.states_value[key] = np.zeros([9,1])\n",
    "        if key_prec not in self.q_player.states_value:\n",
    "            self.q_player.states_value[key_prec] = np.zeros([9,1])\n",
    "        # update    \n",
    "        self.q_player.states_value[key_prec][move_q] += self.q_player.alpha*(r + self.q_player.gamma*max([self.q_player.states_value[key][pos] for pos in self.q_player.empty(state)], default=0)-self.q_player.states_value[key_prec][move_q])  \n",
    "\n",
    "    def play_n_games(self, n, epsilon, train_freq = 250, eps_opponent_train = 0.5, ntt = 0, get_reward = True):\n",
    "        \"\"\"\n",
    "        Function to play n games and compute average reward during training.\n",
    "        INPUTS:\n",
    "        - n: number of games to play\n",
    "        - epsilon: vector of exploration levels of Q-player\n",
    "        - train_freq: frequence of computation of the average reward during training\n",
    "        - eps_opponent_train: epsilon of the opponent player for training (default to 0.5)\n",
    "        - ntt: total number of training games (default to 0)\n",
    "        - get_reward: true if we want to store the averaged reward\n",
    "        \"\"\"\n",
    "        # initializing the variable for the cumulated reward\n",
    "        r = 0\n",
    "        for i in range(n):\n",
    "            # since X always plays first, we need to exchange the id corresponding to the player each time\n",
    "            self.q_player.set_player('O',i)\n",
    "            self.opponent.set_player('X',i)\n",
    "            # store reward of the current game\n",
    "            r += self.game_train(epsilon[i+ntt], eps_opponent = eps_opponent_train)\n",
    "            # every 250 games, compute and store the averaged reward (if needed)\n",
    "            if (i+1)%train_freq == 0 and i != 0 and get_reward:\n",
    "                self.reward_vec.append(r/train_freq)\n",
    "                r = 0 # clean the variable for the cumulated sum\n",
    "\n",
    "    def test(self,n_test,epsilon_opp):\n",
    "        \"\"\"\n",
    "        Function to play n_test games.\n",
    "        INPUTS:\n",
    "        - n_test: number of test games\n",
    "        - epsilon: exploration level of the opponent\n",
    "        OUTPUTS:\n",
    "        - M/n_test: averaged reward\n",
    "        \"\"\"\n",
    "        M = 0 # variable to store the cumulated reward\n",
    "        for i in range(n_test):\n",
    "            # switching the first player at each game\n",
    "            self.q_player.set_player('O',i)\n",
    "            self.opponent.set_player('X',i)\n",
    "            # playing a game\n",
    "            new_game = self.game_test(0, eps_opponent = epsilon_opp)\n",
    "            M += new_game\n",
    "        return M/n_test\n",
    "                \n",
    "    def play_train_test(self, epsilon, eps_opponent_train = 0.5, n_train = 250, n_tot_train = 20000, n_test = 500):\n",
    "        \"\"\"\n",
    "        Function to play n_tot_train games and compute average reward every n_train games with n_test test games.\n",
    "        INPUTS:\n",
    "        - epsilon: exploration level of Q-player\n",
    "        - eps_opponent_train: epsilon of the opponent (default to 0.5)\n",
    "        - n_train: frequence of performing the testing during training\n",
    "        - n_tot_train: total number of training games\n",
    "        - n_test: number of games to test with\n",
    "        OUTPUTS:\n",
    "        - M_opt_vec: array with averaged rewards on n_test games against the OPTIMAL player, computed each n_train games during training\n",
    "        - M_rand_vec: array with averaged rewards on n_test games against the OPTIMAL player, computed each n_train games during training\n",
    "        \"\"\"\n",
    "        ntt = 0 # index of training game\n",
    "        M_opt_vec = []\n",
    "        M_rand_vec = []\n",
    "        print_time = True\n",
    "        if print_time :\n",
    "            start = time.time()\n",
    "        while ntt < n_tot_train:\n",
    "            # play n_train training games\n",
    "            self.play_n_games(n_train , epsilon, eps_opponent_train = eps_opponent_train, ntt = ntt, get_reward = False)\n",
    "            # each n_train games, perform the test\n",
    "            if ntt!=0 and ntt%n_train == 0 :\n",
    "                M_rand_vec.append(self.test(n_test,1))\n",
    "                M_opt_vec.append(self.test(n_test,0))\n",
    "                if print_time and M_rand_vec[-1]>= self.M_rand_threshold and M_opt_vec[-1]>=self.M_opt_threshold :\n",
    "                    end = time.time()\n",
    "                    print('Converged in: ', end-start)\n",
    "                    print_time = False\n",
    "\n",
    "            # update the effective number of training games\n",
    "            ntt += n_train\n",
    "            \n",
    "        return M_opt_vec, M_rand_vec\n",
    "                    \n",
    "            \n",
    "    def plot_avg_reward(self, n, epsilon, step=250):\n",
    "        \"\"\"\n",
    "        Plotting average reward during training, for a specific value of exploration level.\n",
    "        INPUTS:\n",
    "        - n: number of games\n",
    "        - epsilon: exploration level of the q_player\n",
    "        - step: frequency of the samples\n",
    "        \"\"\"\n",
    "        xx = np.arange(step,n+1,step)\n",
    "        yy = self.reward_vec\n",
    "        fig = plt.figure() # opens a new figure for each epsilon\n",
    "        plt.plot(xx,yy, label = \"epsilon=\"+str(epsilon))\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.xlabel(\"Nb of games\")\n",
    "        plt.ylabel(\"Avg reward\")\n",
    "        fig.savefig(\"ploteps_\"+str(epsilon)+\".png\")\n",
    "        \n",
    "    def plot_N_star_reward(self, N_star, n, rewards, step=250):\n",
    "        \"\"\"\n",
    "        Plotting average reward during training, for different values of N_star.\n",
    "        INPUTS:\n",
    "        - N_star: N_star\n",
    "        - n: number of games\n",
    "        - rewards: vector of rewards computed each n \n",
    "        - step: frequency of the samples\n",
    "        \"\"\"\n",
    "        xx = np.arange(step,n+1,step)\n",
    "        fig = plt.figure() # one figure for each N*\n",
    "        for i in range(len(N_star)):\n",
    "            plt.plot(xx,rewards[i], label = 'N*='+str(N_star[i]), linewidth=1, )\n",
    "        plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\") \n",
    "        plt.grid()\n",
    "        plt.xlabel(\"Nb of games\")\n",
    "        plt.ylabel(\"Avg reward\")\n",
    "        fig.savefig(\"plot_nstar_ql.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 1\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "np.random.seed(2022)\n",
    "random.seed(2023)\n",
    "eps_vector = [0.1,0.2,0.3,0.4,0.5] # vector of epsilons to try\n",
    "rew_epsilon = []\n",
    "\n",
    "N = 20000\n",
    "for epsilon in eps_vector:\n",
    "    game = QLearning(epsilon,0.5)\n",
    "    game.q_player.states_value = dict() # inizialize Q-value table\n",
    "    game.play_n_games(N, epsilon*np.ones(N), eps_opponent_train = 0.5)\n",
    "    game.plot_avg_reward(N,epsilon)\n",
    "    rew_epsilon.append(game.reward_vec)\n",
    "    print('epsilon=' + str(epsilon) + ' Mean reward: ' + str(np.mean(game.reward_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "np.random.seed(2022)\n",
    "random.seed(2023)\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "rew_N_star = []\n",
    "N = 20000\n",
    "N_star_vec = [1, 100, 1000, 10000, 15000, 20000, 30000,40000] # vector of N_star to try\n",
    "for N_star in N_star_vec :\n",
    "    epsilon = []\n",
    "    for i in np.arange(1,N+1):\n",
    "        epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "    game = QLearning(0,0.5)\n",
    "    game.q_player.states_value = dict()\n",
    "    game.play_n_games(N,epsilon)\n",
    "    rew_N_star.append(game.reward_vec)\n",
    "    print('N*='+str(N_star)+ ' Mean reward: ' + str(np.mean(game.reward_vec[40:])))\n",
    "game.plot_N_star_reward(N_star_vec,N,rew_N_star)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 3\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "np.random.seed(2022)\n",
    "random.seed(2023)\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "N = 20000 # number of training games\n",
    "N_star_vec = [1,10,100, 1000, 10000, 15000, 20000, 30000,40000] # vector of N_star to try\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "for N_star in N_star_vec :\n",
    "    # generating the vector of decreasing explorations\n",
    "    epsilon = []\n",
    "    for i in np.arange(1,N+1):\n",
    "        epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "    # initializing an instance of the QLearning class with the generated epsilon vector\n",
    "    game = QLearning(epsilon, 0.5)\n",
    "    game.q_player.states_value = dict() # cleaning the Q-value table\n",
    "    M_opt_vec, M_rand_vec = game.play_train_test(epsilon, n_train = 250, n_tot_train = N, n_test = 500)\n",
    "    # printing M_opt and M_rand means (after half of the games) for the corresponding N*\n",
    "    print('N_star=' + str(N_star) + ' M_opt mean: ' + str(np.mean(M_opt_vec[40:])))\n",
    "    print('N_star=' + str(N_star) + ' M_rand mean: ' + str(np.mean(M_rand_vec[40:])))\n",
    "    # plotting M_opt and M_rand for the corresponding N*\n",
    "    ax1.plot(np.arange(1,len(M_opt_vec)+1)*250, M_opt_vec, label = 'N*='+str(N_star))\n",
    "    ax1.set_title('M_opt')\n",
    "    plt1.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "    ax1.grid()\n",
    "    ax2.plot(np.arange(1,len(M_rand_vec)+1)*250, M_rand_vec, label = 'N*='+str(N_star))\n",
    "    plt2.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "    ax2.grid()\n",
    "    ax2.set_title('M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 4\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "np.random.seed(2022)\n",
    "random.seed(2022)\n",
    "epsilon = []\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "N = 20000\n",
    "N_star = 100 # choice of the best N_star\n",
    "\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "\n",
    "# generation of the vector with decreasing exploration levels\n",
    "for i in np.arange(1,N+1):\n",
    "    epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "    \n",
    "eps_opt = [0, 0.2, 0.4, 0.6, 0.8, 1] # vector of eps_opt to try\n",
    "for eps in eps_opt:\n",
    "    game = QLearning(0, eps)\n",
    "    game.q_player.states_value = dict() # cleaning the Q-value table\n",
    "    M_opt_vec, M_rand_vec = game.play_train_test(epsilon, eps_opponent_train = eps, n_train = 250, n_tot_train = N, n_test = 500)\n",
    "    # printing mean of M_opt and max & mean of M_rand for comparison among different eps_opt\n",
    "    print('epsilon=' + str(eps) + ' M_opt mean: ' + str(np.mean(M_opt_vec[40:])))\n",
    "    print('epsilon=' + str(eps) + ' M_rand max: ' + str(np.max(M_rand_vec)))\n",
    "    print('epsilon=' + str(eps) + ' M_rand mean: ' + str(np.mean(M_rand_vec[40:])))\n",
    "    ax1.plot(np.arange(1,len(M_opt_vec)+1)*250, M_opt_vec, label = 'eps='+str(eps))\n",
    "    ax2.plot(np.arange(1,len(M_rand_vec)+1)*250, M_rand_vec, label = 'eps='+str(eps))\n",
    "       \n",
    "plt1.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "ax1.grid()\n",
    "ax1.set_title('M_opt')  \n",
    "plt2.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "ax2.grid()\n",
    "ax2.set_title('M_rand') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfQLearning:\n",
    "    \"Class for self-learning\"\n",
    "    def __init__(self, epsilon_q, epsilon_opp = 0.5, alpha = 0.05, gamma = 0.99):\n",
    "        \"\"\"\n",
    "        Initializer\n",
    "        INPUTS:\n",
    "        - epsilon1: exploration level of q_player\n",
    "        - epsilon2: exploration level of opponent\n",
    "        - alpha: learning rate\n",
    "        - gamma: discount factor\n",
    "        \"\"\"\n",
    "        self.q_player1 = QPlayer(epsilon_q, alpha, gamma, player_name = 'O')\n",
    "        self.q_player2 = QPlayer(epsilon_q, alpha, gamma, player_name = 'X')\n",
    "        self.opponent = OptimalPlayer(epsilon_opp)\n",
    "        self.env = TictactoeEnv()\n",
    "        self.reward_vec = []\n",
    "\n",
    "        # thresholds for computing training time\n",
    "        self.M_opt_threshold = -0.044 * 1.2 # this is a negative value, so we cannot multiply by 0.8!\n",
    "        self.M_rand_threshold = 0.866 * 0.8\n",
    "        \n",
    "    def game_train(self, eps_q_player):\n",
    "        \"\"\"\n",
    "        Function to play a training game.\n",
    "        INPUTS:\n",
    "        - eps_q_player: exploration level of q_player\n",
    "        - eps_opponent: exploration level of opponent (default = 0.5)\n",
    "        \"\"\"\n",
    "\n",
    "        self.env.reset() # reset the board\n",
    "        \n",
    "        states = [] # to store states during the game\n",
    "        states.append(self.env.grid.copy())\n",
    "        \n",
    "        # setting exploration level\n",
    "        self.q_player1.epsilon = eps_q_player \n",
    "        self.q_player2.epsilon = eps_q_player\n",
    "        \n",
    "        move1 = -1 # initializing the moves\n",
    "        move2 = -1\n",
    "        \n",
    "        while not self.env.end : # while the game is not over\n",
    "            if self.env.current_player == self.q_player1.player_name :   \n",
    "                # action of q_player 1\n",
    "                move1 = self.q_player1.act(states[-1])\n",
    "                self.env.step(move1)\n",
    "                    \n",
    "            else : \n",
    "                # action of q_player 2\n",
    "                move2 = self.q_player2.act(states[-1])\n",
    "                self.env.step(move2)\n",
    "                \n",
    "            states.append(self.env.grid.copy()) # storage of the grid after the move        \n",
    "\n",
    "            # update the q_values before the move of the q_player, after the first move    \n",
    "            if self.env.current_player == self.q_player1.player_name and move1 != -1: \n",
    "                self.update_q(self.q_player1.player_name, states[-3], states[-1], move1)\n",
    "            if self.env.current_player == self.q_player2.player_name and move2 != -1: \n",
    "                self.update_q(self.q_player2.player_name, states[-3], states[-1], move2)\n",
    "\n",
    "        # to avoid missing one case of update, when the game ends after the move of the q-player        \n",
    "        if self.env.end and self.env.current_player == self.q_player2.player_name :\n",
    "            self.update_q(self.q_player1.player_name, states[-2], states[-1], move1)\n",
    "        if self.env.end and self.env.current_player == self.q_player1.player_name :\n",
    "            self.update_q(self.q_player2.player_name, states[-2], states[-1], move2)\n",
    "        \n",
    "        \n",
    "    def update_q(self, player_name, state_prec, state, move_q):\n",
    "        \"\"\"\n",
    "        Function updating the table of Q-values\n",
    "        INPUT:\n",
    "        - player: id of the player\n",
    "        - states: array with states visited by the agent\n",
    "        - actions: array with actions performed by the agent\n",
    "        \"\"\"\n",
    "        r = self.env.reward(player_name) # get the reward of the q_player \n",
    "        \n",
    "        # the Q-values table of the terminal state is initialized with zeros\n",
    "        if player_name == self.q_player1.player_name:\n",
    "            key = self.q_player1.correct_grid(state)\n",
    "            key_prec = self.q_player1.correct_grid(state_prec)\n",
    "        else:\n",
    "            key = self.q_player2.correct_grid(state)\n",
    "            key_prec = self.q_player2.correct_grid(state_prec)\n",
    "\n",
    "        # check if the state is already present \n",
    "        if key not in self.q_player1.states_value:\n",
    "            self.q_player1.states_value[key] = np.zeros([9,1])\n",
    "        if key_prec not in self.q_player1.states_value:\n",
    "            self.q_player1.states_value[key_prec] = np.zeros([9,1])\n",
    "            \n",
    "        # update Q(s,a)\n",
    "        self.q_player1.states_value[key_prec][move_q] += self.q_player1.alpha*(r + self.q_player1.gamma*max([self.q_player1.states_value[key][pos] for pos in self.q_player1.empty(state)], default=0)-self.q_player1.states_value[key_prec][move_q])  \n",
    "       \n",
    "    def game_test(self, eps_q_player, eps_opponent = 0):\n",
    "        \"\"\"\n",
    "        Function to play a test game\n",
    "        INPUTS:\n",
    "        - eps_q_player: exploration level of q_player\n",
    "        - eps_opponent: exploration level of opponent (default = 0.5)\n",
    "        \"\"\"\n",
    "        self.env.reset() # reset the board\n",
    "\n",
    "        #setting exploration levels\n",
    "        self.q_player1.epsilon = eps_q_player\n",
    "        self.opponent.epsilon = eps_opponent\n",
    "        \n",
    "        while not self.env.end: # while the game is not over\n",
    "            \n",
    "            if self.env.current_player == self.q_player1.player_name :\n",
    "                # action of the q_player\n",
    "                move_q = self.q_player1.act(self.env.grid)\n",
    "                self.env.step(move_q)\n",
    "            else:\n",
    "                # action of the opponent\n",
    "                move_opp = self.opponent.act(self.env.grid)\n",
    "                self.env.step(move_opp)\n",
    "\n",
    "        # return reward of the q-player\n",
    "        return self.env.reward(self.q_player1.player_name)\n",
    "            \n",
    "    def play_n_games(self, n, epsilon, ntt = 0):\n",
    "        \"\"\"\n",
    "        Function to play n training games.\n",
    "        INPUTS:\n",
    "        - n: number of games to play\n",
    "        - epsilon: vector of exploration levels of Q-player\n",
    "        - ntt: total number of training games (default to 0)\n",
    "        \"\"\"\n",
    "        for i in range(n):\n",
    "            self.q_player1.set_player('O',i)\n",
    "            self.q_player2.set_player('X',i+1)\n",
    "            self.game_train(epsilon[i+ntt])\n",
    "                \n",
    "    def play_train_test(self, epsilon, n_train = 250, n_tot_train = 20000, n_test = 500):\n",
    "        \"\"\"\n",
    "        Function to play n_tot_train games and compute average reward every n_train games with n_test test games.\n",
    "        INPUTS:\n",
    "        - epsilon: exploration level of Q-player\n",
    "        - n_train: frequence of performing the testing during training\n",
    "        - n_tot_train: total number of training games\n",
    "        - n_test: number of games to test with\n",
    "        OUTPUTS:\n",
    "        - M_opt_vec: array with averaged rewards on n_test games against the OPTIMAL player, computed each n_train games during training\n",
    "        - M_rand_vec: array with averaged rewards on n_test games against the OPTIMAL player, computed each n_train games during training\n",
    "        \"\"\"\n",
    "        ntt = 0 # index of training game\n",
    "        M_opt_vec = []\n",
    "        M_rand_vec = []\n",
    "        print_time = True\n",
    "        if print_time :\n",
    "            start = time.time()\n",
    "        while ntt < n_tot_train:\n",
    "            # play n_train training games\n",
    "            self.play_n_games(n_train, epsilon, ntt)\n",
    "            # each n_train games, perform the test\n",
    "            if ntt!=0 and ntt%n_train == 0 :\n",
    "                M_opt_vec.append(self.test(n_test,0))\n",
    "                M_rand_vec.append(self.test(n_test,1))\n",
    "                if print_time and M_rand_vec[-1]>= self.M_rand_threshold and M_opt_vec[-1]>=self.M_opt_threshold :\n",
    "                    end = time.time()\n",
    "                    print('Converged in: ', end-start)\n",
    "                    print_time = False\n",
    "            # update the effective number of training games\n",
    "            ntt += n_train\n",
    "            \n",
    "        return M_opt_vec, M_rand_vec\n",
    "    \n",
    "    def test(self,n_test,epsilon_opp):\n",
    "        \"\"\"\n",
    "        Function to play n_test games.\n",
    "        INPUTS:\n",
    "        - n_test: number of test games\n",
    "        - epsilon_opp: exploration level of the opponent\n",
    "        OUTPUTS:\n",
    "        - M/n_test: averaged reward\n",
    "        \"\"\"\n",
    "        M = 0 # variable to store the cumulated reward\n",
    "        for i in range(n_test):\n",
    "            # switching the first player at each game\n",
    "            self.q_player1.set_player('O',i)\n",
    "            self.opponent.set_player('X',i)\n",
    "            # playing a game\n",
    "            new_game = self.game_test(0, eps_opponent = epsilon_opp)\n",
    "            M += new_game\n",
    "        return M/n_test\n",
    "                \n",
    "        \n",
    "    def plot_N_star_reward(self,N_star, n, rewards, step = 250):\n",
    "        \"\"\"\n",
    "        Plotting average reward during training, for different values of N_star.\n",
    "        INPUTS:\n",
    "        - N_star: N_star\n",
    "        - n: number of games\n",
    "        - rewards: vector of rewards computed each n \n",
    "        - step: frequency of the samples\n",
    "        \"\"\"\n",
    "        xx = np.arange(step,n+1,step)\n",
    "        fig = plt.figure() # one figure for each N*\n",
    "        for i in range(len(N_star)):\n",
    "            plt.plot(xx,rewards[i], label = 'N*='+str(N_star[i]))\n",
    "        plt.legend()\n",
    "        plt.xlabel(\"Nb of games\")\n",
    "        plt.ylabel(\"Avg reward\")\n",
    "        plt.grid()\n",
    "        fig.savefig(\"plot_nstar_selfql.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 7\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "np.random.seed(2022)\n",
    "random.seed(2023)\n",
    "eps_vector = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6] # vector of epsilons to try\n",
    "rew_epsilon = []\n",
    "\n",
    "\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "\n",
    "ax1.set_title('M_opt')\n",
    "ax2.set_title('M_rand')\n",
    "\n",
    "for epsilon in eps_vector:\n",
    "    game = SelfQLearning(epsilon)\n",
    "    game.q_player1.states_value = dict() # inizialize Q-value table\n",
    "    M_opt_vec, M_rand_vec = game.play_train_test(epsilon*np.ones(20000), n_train = 250, n_tot_train = 20000, n_test = 500)\n",
    "    ax1.plot(np.arange(1,len(M_opt_vec)+1)*250, M_opt_vec, label = 'eps='+str(epsilon))\n",
    "    ax2.plot(np.arange(1,len(M_rand_vec)+1)*250, M_rand_vec, label = 'eps='+str(epsilon))\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "plt1.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "plt2.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 8\n",
    "\n",
    "# setting seeds for reproducibility\n",
    "np.random.seed(2022)\n",
    "random.seed(2023)\n",
    "\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "N = 20000\n",
    "#N_star_vec = [1] #(best N_star, needed for the heatmaps of Q9)\n",
    "N_star_vec = [1, 100, 1000, 10000, 15000, 20000, 30000,40000] # vector of N_star to try\n",
    "\n",
    "for N_star in N_star_vec :\n",
    "    # generating the vector of decreasing explorations\n",
    "    epsilon = []\n",
    "    for i in np.arange(1,N+1):\n",
    "        epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "    # initializing an instance of the QLearning class with the generated epsilon vector\n",
    "    game = SelfQLearning(epsilon)\n",
    "    game.q_player1.states_value = dict() # cleaning the Q-value table\n",
    "    M_opt_vec, M_rand_vec = game.play_train_test(epsilon, n_train = 250, n_tot_train = N, n_test = 500)\n",
    "    # printing M_rand max for the corresponding N*\n",
    "    print('N_star=' + str(N_star) + ' M_opt mean: ' + str(np.mean(M_opt_vec[40:])))\n",
    "    print('N_star=' + str(N_star) + ' M_rand mean: ' + str(np.mean(M_rand_vec[40:])))\n",
    "    # plotting M_opt and M_rand for the corresponding N*\n",
    "    ax1.plot(np.arange(1,len(M_opt_vec)+1)*250, M_opt_vec, label = 'N*='+str(N_star))\n",
    "    ax1.set_title('M_opt')\n",
    "    plt1.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "    ax2.plot(np.arange(1,len(M_rand_vec)+1)*250, M_rand_vec, label = 'N*='+str(N_star))\n",
    "    plt2.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "    ax2.set_title('M_rand')\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "plt1.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "plt2.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 10\n",
    "# first state selected\n",
    "states1 =  '[-1. -1. -0.  1. -0.  1. -1.  1. -1.]'\n",
    "sn.heatmap(game.q_player1.states_value[states1].reshape(3,3), center = 0, square = True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second state selected\n",
    "states2 = '[0. 0. 0. 0. 0. 0. 0. 0. 0.]'\n",
    "sn.heatmap(game.q_player1.states_value[states2].reshape(3,3), center=0, square = True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third state selected\n",
    "states3 = '[-1. -1. -0. -0. -0.  1. -0. -0. -0.]'\n",
    "sn.heatmap(game.q_player1.states_value[states3].reshape(3,3), center = 0, linewidths=.5, square = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package and environment loading\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tic_env import TictactoeEnv, OptimalPlayer\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Net(n_hidden = 128):\n",
    "  \"\"\"\n",
    "  Fully Conncected Neural Network with:\n",
    "  - input of shape (3,3,2)\n",
    "  - two hidden layers with n_hidden hidden neurons and ReLu as activation function\n",
    "  - output layer of shape (1,9) and linear activation function\n",
    "  \"\"\"\n",
    "  # Input layer\n",
    "  inputs = layers.Input(shape=(3,3,2))\n",
    "  flatten = layers.Flatten()(inputs) # flattening a input of (3,3,2) to (18,1)\n",
    "  # Hidden layers\n",
    "  layer1 = layers.Dense(n_hidden,activation=\"relu\")(flatten)\n",
    "  layer2 = layers.Dense(n_hidden,activation=\"relu\")(layer1)\n",
    "  # Output layer\n",
    "  outputs = layers.Dense(9,activation =\"linear\")(layer2)\n",
    "\n",
    "  return keras.Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQPlayer:\n",
    "  \"\"\"\n",
    "  Class implementing the deep Q-player. \n",
    "  ATTRIBUTES: \n",
    "  - epsilon: exploration rate\n",
    "  - player_name: player name (X or O)\n",
    "  - model: net Net()\n",
    "  - model_target: target net Net()\n",
    "  - loss_function: Huber loss\n",
    "  \"\"\"\n",
    "  def __init__(self, epsilon, player_name = 'O'):\n",
    "    self.epsilon = epsilon #exploration rate\n",
    "    self.player_name = player_name\n",
    "    self.model = Net() \n",
    "    self.model_target = Net()\n",
    "    self.loss_function = keras.losses.Huber()\n",
    "\n",
    "  def set_player(self, player_name = 'O', j=-1):\n",
    "        \"\"\"\n",
    "        Function to set player symbol, needed to alternate the starting player.\n",
    "        INPUTS:\n",
    "        - player_name: player symbol on the board\n",
    "        - j : number of the current game\n",
    "        \"\"\"\n",
    "        self.player_name = player_name\n",
    "        if j != -1:\n",
    "            self.player_name = 'O' if j % 2 == 0 else 'X'\n",
    "\n",
    "  def set_epsilon(self, epsilon):\n",
    "        \"\"\"\n",
    "        Functions that sets the exploration level.\n",
    "        INPUTS:\n",
    "        - epsilon: exploration level\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLearning:\n",
    "  \"\"\"\n",
    "  Class implementing the Deep Q-learning algorithm (DQN) against the eps-optimal player\n",
    "  \"\"\"\n",
    "  def __init__(self, epsilon, epsilon_opponent = 0.5, alpha = 1e-3, gamma = 0.99, player_name = 'O', N = 20000, n_train = 250, n_test = 500, buffer_size = 10000, batch_size = 64, test = False):\n",
    "    self.epsilon = epsilon #exploration rate of the q-player \n",
    "    self.alpha = alpha  #learning rate\n",
    "    self.gamma = gamma  #discount factor \n",
    "    self.dq_player = DQPlayer(epsilon) #dq-player\n",
    "    self.opponent = OptimalPlayer(epsilon_opponent) #optimal player \n",
    "    self.env = TictactoeEnv()\n",
    "\n",
    "    \n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate = self.alpha)\n",
    "    self.N = N  #total number of training games\n",
    "    self.n_train = n_train #number of training games between test\n",
    "    self.n_test = n_test #number of games for test\n",
    "    self.test = test #bool, true if we want to compute M_rand and M_opt\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    self.action_history = [] #vector collecting consecutive actions\n",
    "    self.state_history = []  #vector collecting consecutive states \n",
    "    self.state_next_history = [] #vector collecting consecutive (next) states. Differs from the previous for the first and last elements\n",
    "    self.rewards_history = [] #vector collecting consecutive history\n",
    "    self.done_history = [] #vector collecting consecutive values of self.env.end\n",
    "\n",
    "    self.episode_reward_history = [] #vector collecting dq-player rewards \n",
    "    self.loss_history = [] #vector collecting loss values at the end of consecutive games\n",
    "    self.reward_mean = [] \n",
    "    self.loss_mean = []\n",
    "\n",
    "    self.M_rand = []\n",
    "    self.M_opt = []\n",
    "\n",
    "    self.episode_count = 0 # game counter\n",
    "    self.num_actions = 9 # number of actions available\n",
    "\n",
    "    self.max_memory_length = buffer_size # buffer size\n",
    "    self.update_target_network = 500 # how often to update the target network\n",
    "\n",
    "    # thresholds for training time\n",
    "    self.M_rand_threshold = 0.929 * 0.8\n",
    "    self.M_opt_threshold = -0.0141 * 1.2\n",
    "\n",
    "  def build_tensor(self, grid, player_name):\n",
    "      \"\"\"\n",
    "      Functions that builds the tensor from the current grid.\n",
    "      INPUTS:\n",
    "      - grid: current grid\n",
    "      - player_name: name of the dq-player \n",
    "      \"\"\"\n",
    "      my_state = grid.copy()\n",
    "      opp_state = grid.copy()\n",
    "      if player_name == 'X':\n",
    "        my_state[my_state ==-1.] = 0.\n",
    "        opp_state[opp_state==1.] = 0.\n",
    "        opp_state[opp_state==-1.] = 1.\n",
    "      if player_name == 'O':\n",
    "        my_state[my_state==1.] = 0.\n",
    "        my_state[my_state==-1.] = 1.\n",
    "        opp_state[opp_state==-1.] = 0.\n",
    "\n",
    "      my_tstate = tf.convert_to_tensor(my_state)\n",
    "      opp_tstate = tf.convert_to_tensor(opp_state)\n",
    "\n",
    "      ret_tensor = tf.stack([my_tstate,opp_tstate], axis = 2)\n",
    "\n",
    "      ret_tensor = tf.reshape(ret_tensor, shape = [1,3,3,2])\n",
    "\n",
    "      return ret_tensor\n",
    "\n",
    "    \n",
    "  def training(self, eps_opponent = 0.5):\n",
    "    \"\"\"\n",
    "    Functions that runs self.N training games. If self.test = true, it runs self.n_test every self.n_train trainings.\n",
    "    \"\"\"\n",
    "    self.episode_count = 0\n",
    "    print_time = True\n",
    "    start = time.time()\n",
    "    while True:  # Run until solved, i.e. while episode_count < self.N\n",
    "        self.dq_player.set_epsilon(self.epsilon[self.episode_count])\n",
    "        self.opponent.epsilon = eps_opponent\n",
    "        self.episode_count += 1\n",
    "        state,_,_=self.env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        #switch of the starting player\n",
    "        self.dq_player.set_player('O', self.episode_count)\n",
    "        self.opponent.set_player('X', self.episode_count)\n",
    "\n",
    "        #first move if the starting player is the optimal player\n",
    "        if self.env.current_player == self.opponent.player:\n",
    "          move = self.opponent.act(state)\n",
    "          state,_,_ = self.env.step(move)\n",
    "\n",
    "        while not done: # while the game is not over\n",
    "          state_tensor = self.build_tensor(state,self.dq_player.player_name)\n",
    "\n",
    "          #action of the dq-player\n",
    "          if self.dq_player.epsilon > np.random.rand(1)[0]:\n",
    "                  # Take random action\n",
    "                  action = np.random.choice(self.num_actions)\n",
    "          else:\n",
    "                  # Predict action Q-values\n",
    "                  action_values = self.dq_player.model(state_tensor, training=False)\n",
    "                  # Take best action\n",
    "                  idx = np.argwhere(action_values[0].numpy() == np.amax(action_values[0].numpy()))[:,0]\n",
    "                  action = random.choice(idx)\n",
    "\n",
    "          #check if the action is available, otherwise end of the game\n",
    "          try:\n",
    "            state_adv,_,_ = self.env.step(int(action))\n",
    "          except ValueError:\n",
    "              self.env.end = True\n",
    "              self.env.winner = self.opponent.player\n",
    "          if not self.env.end:\n",
    "            action_adv = self.opponent.act(state_adv)\n",
    "            state_next,_,_ = self.env.step(action_adv)\n",
    "          else :\n",
    "            state_next = state\n",
    "          \n",
    "          reward = self.env.reward(self.dq_player.player_name)\n",
    "          episode_reward += reward\n",
    "          done = self.env.end\n",
    "\n",
    "          self.action_history.append(action)\n",
    "          self.state_history.append(self.build_tensor(state, self.dq_player.player_name))\n",
    "          self.state_next_history.append(self.build_tensor(state_next, self.dq_player.player_name))\n",
    "          self.done_history.append(done)\n",
    "          self.rewards_history.append(reward)\n",
    "\n",
    "          state = state_next\n",
    "\n",
    "          #training\n",
    "          if len(self.action_history) > self.batch_size:\n",
    "\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(self.action_history)), size=self.batch_size)\n",
    "            rewards_sample = self.rewards_history[0]\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = tf.squeeze(np.array([self.state_history[i] for i in indices]))\n",
    "            state_next_sample = tf.squeeze(np.array([self.state_next_history[i] for i in indices]))\n",
    "            \n",
    "            if self.batch_size == 1:\n",
    "              state_sample = tf.expand_dims(state_sample, axis = 0)\n",
    "              state_next_sample = tf.expand_dims(state_next_sample, axis = 0)\n",
    "              \n",
    "            rewards_sample = [self.rewards_history[i] for i in indices]\n",
    "            action_sample = [self.action_history[i] for i in indices]\n",
    "\n",
    "            done_sample = tf.convert_to_tensor([float(self.done_history[i]) for i in indices])\n",
    "\n",
    "            masks = tf.one_hot(action_sample, 9)\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                prediction = self.dq_player.model(state_sample)\n",
    "                prediction = tf.reduce_sum(tf.multiply(prediction, masks), axis=1)\n",
    "                future_rewards = self.dq_player.model_target(state_next_sample).numpy()\n",
    "                target = rewards_sample + self.gamma * (1-done_sample) * np.max(future_rewards, axis = 1)\n",
    "\n",
    "\n",
    "                loss = self.dq_player.loss_function(target, prediction)\n",
    "                self.loss_history.append(loss.numpy())\n",
    "\n",
    "                # Backpropagation\n",
    "                grads = tape.gradient(loss, self.dq_player.model.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.dq_player.model.trainable_variables))\n",
    "\n",
    "          if len(self.rewards_history) > self.max_memory_length:\n",
    "                  del self.rewards_history[:1]\n",
    "                  del self.state_history[:1]\n",
    "                  del self.state_next_history[:1]\n",
    "                  del self.action_history[:1]\n",
    "                  del self.done_history[:1]\n",
    "\n",
    "          if done:  #if the game is ended\n",
    "            break\n",
    "\n",
    "        self.episode_reward_history.append(episode_reward)\n",
    "\n",
    "        #mean rewards and test if self.test == true\n",
    "        if self.episode_count % self.n_train == 0:\n",
    "          self.reward_mean.append(np.mean(self.episode_reward_history))\n",
    "          self.episode_reward_history = []\n",
    "          if self.test:\n",
    "            self.test_games(n_test = self.n_test)\n",
    "            if print_time and self.M_rand[-1] > self.M_rand_threshold and self.M_opt[-1] > self.M_opt_threshold :\n",
    "              end = time.time()\n",
    "              print('Converged in ', end-start)\n",
    "              print_time = False\n",
    "            #self.opponent.epsilon = 0.5\n",
    "        \n",
    "        #decreasing learning rate\n",
    "        if self.episode_count % 2500 == 0:\n",
    "          self.alpha = 0.5*self.alpha\n",
    "          self.optimizer = keras.optimizers.Adam(learning_rate = self.alpha)\n",
    "                \n",
    "        if self.episode_count % self.n_train == 0 :\n",
    "          self.loss_mean.append(np.mean(self.loss_history))\n",
    "          self.loss_history = []\n",
    "\n",
    "        if self.episode_count % self.update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            self.dq_player.model_target.set_weights(self.dq_player.model.get_weights())\n",
    "            # Log details\n",
    "            template = \"Update target network at episode {}\"\n",
    "            print(template.format(self.episode_count))\n",
    "\n",
    "            \n",
    "        if self.episode_count >= self.N :\n",
    "          print(\"Done {} training games\".format(self.N))\n",
    "          return self.M_rand, self.M_opt\n",
    "\n",
    "\n",
    "  def test_games(self, eps_dq_player = 0., n_test = 500):\n",
    "      \"\"\"\n",
    "      Function to play n test games\n",
    "      INPUTS:\n",
    "      - eps_dq_player: exploration level of dq_player\n",
    "      - n_test: number of test games\n",
    "      \"\"\"\n",
    "      rewards_rand = 0\n",
    "      rewards_opt = 0\n",
    "      for i in range(n_test):\n",
    "        self.dq_player.set_player('O',i)\n",
    "        self.opponent.set_player('X',i)\n",
    "        new_game = self.game_test(eps_dq_player, 1.)\n",
    "        rewards_rand += new_game      \n",
    "        rewards_opt += self.game_test(eps_dq_player, 0.)\n",
    "      rewards_rand = rewards_rand/n_test\n",
    "      rewards_opt = rewards_opt/n_test\n",
    "      self.M_rand.append(rewards_rand)\n",
    "      self.M_opt.append(rewards_opt)\n",
    "\n",
    "  def game_test(self, eps_dq_player = 0., eps_opponent = 0.):\n",
    "        \"\"\"\n",
    "        Function to play a test game\n",
    "        INPUTS:\n",
    "        - eps_dq_player: exploration level of q_player\n",
    "        - eps_opponent: exploration level of opponent\n",
    "        \"\"\"\n",
    "        self.env.reset() # reset the board\n",
    "        #setting exploration levels\n",
    "        self.dq_player.epsilon = eps_dq_player\n",
    "        self.opponent.epsilon = eps_opponent\n",
    "        while not self.env.end: # while the game is not over\n",
    "            \n",
    "          if self.env.current_player == self.dq_player.player_name :\n",
    "                # action of the q_player\n",
    "                state_tensor = self.build_tensor(self.env.grid.copy(),self.dq_player.player_name)\n",
    "\n",
    "                if self.dq_player.epsilon > np.random.rand(1)[0]: action = np.random.choice(self.num_actions)\n",
    "                else:\n",
    "                  # Predict action Q-values\n",
    "                  action_values = self.dq_player.model_target(state_tensor, training=False)\n",
    "                  # Take best action\n",
    "                  action = tf.argmax(action_values[0]).numpy()\n",
    "                try:\n",
    "                  self.env.step(int(action))\n",
    "                except ValueError:\n",
    "                    self.env.end = True\n",
    "                    self.env.winner = self.opponent.player\n",
    "          else:\n",
    "              # action of the opponent\n",
    "              move_opp = self.opponent.act(self.env.grid)\n",
    "              self.env.step(move_opp)\n",
    "\n",
    "        # return reward of the q-player\n",
    "        return self.env.reward(self.dq_player.player_name)\n",
    "\n",
    "\n",
    "  def plots(self):\n",
    "    \"\"\"\n",
    "    Function to plots avg loss and reward\n",
    "    \"\"\"    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,3))\n",
    "    xx = np.arange(self.n_train,self.N+1,self.n_train)\n",
    "    y1 = self.loss_mean\n",
    "    y2 = self.reward_mean\n",
    "    plt.figure()\n",
    "    ax1.plot(xx,y1, label = \"epsilon=\"+str(self.dq_player.epsilon))\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "    ax1.set_xlabel(\"Nb of games\")\n",
    "    ax1.set_ylabel(\"Avg loss\")\n",
    "    ax2.plot(xx,y2, label = \"epsilon=\"+str(self.dq_player.epsilon))\n",
    "    ax2.legend()\n",
    "    ax2.grid()\n",
    "    ax2.set_xlabel(\"Nb of games\")\n",
    "    ax2.set_ylabel(\"Avg reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 11\n",
    "random.seed(2022)\n",
    "np.random.seed(2023)\n",
    "N = 20000\n",
    "epsilon = 0.1*np.ones([N,1])\n",
    "dqn = DQLearning(epsilon = epsilon)\n",
    "dqn.training()\n",
    "dqn.plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 12\n",
    "random.seed(2022)\n",
    "np.random.seed(2023)\n",
    "batch_size = 1\n",
    "buffer_size = 1\n",
    "N = 20000\n",
    "epsilon = 0.1 * np.ones([N,1])\n",
    "dqn = DQLearning(epsilon = epsilon, buffer_size = buffer_size, batch_size = batch_size)\n",
    "dqn.training()\n",
    "dqn.plots()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 13\n",
    "random.seed(2022)\n",
    "np.random.seed(2023)\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "N = 20000\n",
    "N_star_vec = [1,10,100, 1000, 10000, 15000, 20000, 30000,40000]\n",
    "\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "\n",
    "for N_star in N_star_vec:\n",
    "  epsilon = []\n",
    "  for i in np.arange(1,N+1):\n",
    "      epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "\n",
    "  dqn = DQLearning(epsilon = epsilon, test = True)\n",
    "  M_rand, M_opt = dqn.training()\n",
    "  print('N_star =' + str(N_star) + ' M_opt mean ' + str(np.mean(M_opt[40:])))\n",
    "  print('N_star =' + str(N_star) + ' M_rand mean ' + str(np.mean(M_rand[40:])))\n",
    "  ax1.plot(np.arange(1,len(M_opt)+1)*250, M_opt, label = 'N*='+str(N_star))\n",
    "  ax1.set_title('M_opt')\n",
    "  plt1.legend(bbox_to_anchor=(0.88,0.8), loc=\"upper left\")\n",
    "  ax1.grid()\n",
    "  ax2.plot(np.arange(1,len(M_rand)+1)*250, M_rand, label = 'N*='+str(N_star))\n",
    "  plt2.legend(bbox_to_anchor=(0.9,0.8), loc=\"upper left\")\n",
    "  ax2.grid()\n",
    "  ax2.set_title('M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 14\n",
    "random.seed(2022)\n",
    "np.random.seed(2023)\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "N = 20000\n",
    "N_star = 10000\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "epsilon = []\n",
    "for i in np.arange(1,N+1):\n",
    "    epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "\n",
    "eps_opt = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "for eps in eps_opt:\n",
    "    dqn = DQLearning(epsilon_opponent = eps, epsilon = epsilon, test = True)\n",
    "    M_rand, M_opt = dqn.training(eps_opponent = eps)\n",
    "    print('epsilon=' + str(eps) + ' M_opt mean ' + str(np.mean(M_opt[40:])))\n",
    "    print('epsilon=' + str(eps) + ' M_rand max ' + str(np.max(M_rand)))\n",
    "    print('epsilon=' + str(eps) + ' M_rand mean ' + str(np.mean(M_rand[40:])))\n",
    "    ax1.plot(np.arange(1,len(M_opt)+1)*250, M_opt, label = 'eps='+str(eps))\n",
    "    ax2.plot(np.arange(1,len(M_rand)+1)*250, M_rand, label = 'eps='+str(eps))\n",
    "       \n",
    "plt1.legend(bbox_to_anchor=(0.88,0.8), loc=\"upper left\")\n",
    "ax1.grid()\n",
    "ax1.set_title('M_opt')  \n",
    "plt2.legend(bbox_to_anchor=(0.9,0.8), loc=\"upper left\")\n",
    "ax2.grid()\n",
    "ax2.set_title('M_rand') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfDQLearning:\n",
    "  \"\"\"\n",
    "  Class implementing the Deep Q-learning algorithm (DQN) for the agent playing against itself\n",
    "  \"\"\"\n",
    "  def __init__(self, epsilon, epsilon_opponent = 0.5, alpha = 1e-3, gamma = 0.99, player_name = 'O', N = 20000, n_train = 250, n_test = 500, buffer_size = 10000, batch_size = 64, test = True):\n",
    "    self.epsilon = epsilon #exploration rate\n",
    "    self.alpha = alpha #learning rate\n",
    "    self.gamma = gamma #discount factor\n",
    "    self.dq_player1 = DQPlayer(epsilon) #dq player 1\n",
    "    self.dq_player2 = DQPlayer(epsilon) #dq player 2\n",
    "    self.opponent = OptimalPlayer(epsilon_opponent) #optimal player, for test\n",
    "    self.env = TictactoeEnv() \n",
    "\n",
    "    self.optimizer = keras.optimizers.Adam(learning_rate = self.alpha)\n",
    "    self.N = N #total number of training games\n",
    "    self.n_train = n_train #number of training games between test\n",
    "    self.n_test = n_test #number of test games\n",
    "    self.test = test #bool, true if you want to compute M_rand and M_opt\n",
    "    self.batch_size = batch_size\n",
    "    \n",
    "    self.action_history = [] #vector collecting consecutive actions\n",
    "    self.state_history = []  #vector collecting consecutive states \n",
    "    self.state_next_history = [] #vector collecting consecutive (next) states.\n",
    "    self.rewards_history = [] #vector collecting consecutive history\n",
    "    self.done_history = [] #vector collecting consecutive values of self.env.end\n",
    "\n",
    "    self.M_rand = []\n",
    "    self.M_opt = []\n",
    "\n",
    "    self.episode_count = 0 # game counter\n",
    "    self.num_actions = 9 # number of actions available\n",
    "\n",
    "    self.max_memory_length = buffer_size # buffer size\n",
    "    self.update_target_network = 500 # how often to update the target network\n",
    "    self.model = Net()\n",
    "    self.model_target = Net()\n",
    "    self.loss_function = keras.losses.Huber() # Huber loss\n",
    "\n",
    "    self.M_rand_threshold = 0.866 * 0.8\n",
    "    self.M_opt_threshold = -0.0769 * 1.2\n",
    "\n",
    "  def build_tensor(self, grid, player_name):\n",
    "      \"\"\"\n",
    "      Functions that builds the tensor from the current grid.\n",
    "      INPUTS:\n",
    "      - grid: current grid\n",
    "      - player_name: name of the dq-player \n",
    "      \"\"\"\n",
    "      my_state = grid.copy()\n",
    "      opp_state = grid.copy()\n",
    "      if player_name == 'X':\n",
    "        my_state[my_state ==-1.] = 0.\n",
    "        opp_state[opp_state==1.] = 0.\n",
    "        opp_state[opp_state==-1.] = 1.\n",
    "      if player_name == 'O':\n",
    "        my_state[my_state==1.] = 0.\n",
    "        my_state[my_state==-1.] = 1.\n",
    "        opp_state[opp_state==-1.] = 0.\n",
    "\n",
    "      my_tstate = tf.convert_to_tensor(my_state)\n",
    "      opp_tstate = tf.convert_to_tensor(opp_state)\n",
    "\n",
    "      ret_tensor = tf.stack([my_tstate,opp_tstate], axis = 2)\n",
    "\n",
    "      ret_tensor = tf.reshape(ret_tensor, shape = [1,3,3,2])\n",
    "\n",
    "      return ret_tensor\n",
    "\n",
    "    \n",
    "  def training(self):\n",
    "    \"\"\"\n",
    "    Functions that runs self.N training games. If self.test = true, it runs self.n_test every self.n_train trainings.\n",
    "    \"\"\"\n",
    "    self.episode_count = 0\n",
    "    print_time = True\n",
    "    start = time.time()\n",
    "    while True:  # Run until solved, i.e. while episode_count < N\n",
    "        self.dq_player1.set_epsilon(self.epsilon[self.episode_count])\n",
    "        self.dq_player2.set_epsilon(self.epsilon[self.episode_count])\n",
    "        self.episode_count += 1\n",
    "        done = False\n",
    "\n",
    "        #switch of the player names\n",
    "        self.dq_player1.set_player('O', self.episode_count)\n",
    "        self.dq_player2.set_player('X', self.episode_count+1)\n",
    "        \n",
    "        #first state = empty grid\n",
    "        state1,_,_=self.env.reset()\n",
    "        state_next1 = state1 #later re-defined\n",
    "        current_player = self.env.current_player\n",
    "        state_tensor = self.build_tensor(state1, self.env.current_player)\n",
    "\n",
    "        #choice of the first action\n",
    "        if self.dq_player1.epsilon > np.random.rand(1)[0]:\n",
    "                # Take random action\n",
    "                action1 = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "                # Predict action Q-values\n",
    "                action_values = self.model(state_tensor, training=False)\n",
    "                # Take best action\n",
    "                idx = np.argwhere(action_values[0].numpy() == np.amax(action_values[0].numpy()))[:,0]\n",
    "                action1 = random.choice(idx)\n",
    "\n",
    "        state2,_,_ = self.env.step(int(action1))\n",
    "        state_tensor = self.build_tensor(state2, self.env.current_player)\n",
    "\n",
    "        #choice of the second action\n",
    "        if self.dq_player1.epsilon > np.random.rand(1)[0]:\n",
    "                # Take random action\n",
    "                action2 = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "                # Predict action Q-values\n",
    "                action_values = self.model(state_tensor, training=False)\n",
    "                # Take best action\n",
    "                idx = np.argwhere(action_values[0].numpy() == np.amax(action_values[0].numpy()))[:,0]\n",
    "                action2 = random.choice(idx)\n",
    "\n",
    "\n",
    "        while not done: # while the game is not over\n",
    "\n",
    "          not_current_player = np.setdiff1d([\"X\",\"O\"], [current_player])[0] \n",
    "          \n",
    "          #check if action2 is available, otherwise self.env.end = true\n",
    "          try:\n",
    "            state_next1,_,_ = self.env.step(int(action2))\n",
    "          except ValueError:\n",
    "            self.env.end = True\n",
    "            self.env.winner = current_player  #set the winner to be the opposite of the current player        \n",
    "\n",
    "          if self.env.end: \n",
    "            self.action_history.append(action2)\n",
    "            self.state_history.append(self.build_tensor(state2, not_current_player))\n",
    "            self.state_next_history.append(self.build_tensor(self.env.grid, not_current_player))\n",
    "            self.done_history.append(self.env.end)\n",
    "            self.rewards_history.append(self.env.reward(not_current_player))\n",
    "\n",
    "\n",
    "          done = self.env.end\n",
    "          reward = self.env.reward(current_player)\n",
    "          self.action_history.append(action1)\n",
    "          self.state_history.append(self.build_tensor(state1, current_player))\n",
    "          self.state_next_history.append(self.build_tensor(state_next1, current_player))\n",
    "          self.done_history.append(self.env.end)\n",
    "          self.rewards_history.append(self.env.reward(current_player))\n",
    "\n",
    "          #updates of the actions and states\n",
    "          action1 = action2\n",
    "          state1 = state2 \n",
    "          state2 = state_next1\n",
    "          if not done: \n",
    "            state_tensor = self.build_tensor(state2, current_player)\n",
    "\n",
    "          current_player = not_current_player\n",
    "\n",
    "          #next action  \n",
    "          if not self.env.end:\n",
    "            if self.dq_player1.epsilon > np.random.rand(1)[0]:\n",
    "                    # Take random action\n",
    "                    action2 = np.random.choice(self.num_actions)\n",
    "            else:\n",
    "                    # Predict action Q-values\n",
    "                    action_values = self.model(state_tensor, training=False)\n",
    "                    # Take best action\n",
    "                    idx = np.argwhere(action_values[0].numpy() == np.amax(action_values[0].numpy()))[:,0]\n",
    "                    action2 = random.choice(idx)\n",
    "\n",
    "          #training \n",
    "          if len(self.action_history) > self.batch_size:\n",
    "\n",
    "              # Get indices of samples for replay buffers\n",
    "              indices = np.random.choice(range(len(self.action_history)), size=self.batch_size)\n",
    "              rewards_sample = self.rewards_history[0]\n",
    "\n",
    "              # Using list comprehension to sample from replay buffer\n",
    "\n",
    "              state_sample = tf.squeeze(np.array([self.state_history[i] for i in indices]))\n",
    "              state_next_sample = tf.squeeze(np.array([self.state_next_history[i] for i in indices]))\n",
    "              if self.batch_size == 1:\n",
    "                state_sample = tf.expand_dims(state_sample, axis = 0)\n",
    "                state_next_sample = tf.expand_dims(state_next_sample, axis = 0)\n",
    "                \n",
    "              rewards_sample = [self.rewards_history[i] for i in indices]\n",
    "              action_sample = [self.action_history[i] for i in indices]\n",
    "\n",
    "              done_sample = tf.convert_to_tensor([float(self.done_history[i]) for i in indices])\n",
    "\n",
    "              masks = tf.one_hot(action_sample, 9)\n",
    "              \n",
    "              with tf.GradientTape() as tape:\n",
    "                  prediction = self.model(state_sample)\n",
    "                  prediction = tf.reduce_sum(tf.multiply(prediction, masks), axis=1)\n",
    "                  future_rewards = self.model_target(state_next_sample).numpy()\n",
    "                  target = rewards_sample + self.gamma * (1-done_sample) * np.max(future_rewards, axis = 1)\n",
    "                  loss = self.loss_function(target, prediction)\n",
    "                  # Backpropagation\n",
    "                  grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "                  self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "          if len(self.rewards_history) > self.max_memory_length:\n",
    "            del self.rewards_history[:1]\n",
    "            del self.state_history[:1]\n",
    "            del self.state_next_history[:1]\n",
    "            del self.action_history[:1]\n",
    "            del self.done_history[:1]\n",
    "          \n",
    "          if done: \n",
    "            break\n",
    "        #update target model\n",
    "        if self.episode_count % self.update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            # Log details\n",
    "            template = \"Update target network at episode {}\"\n",
    "            print(template.format(self.episode_count))\n",
    "\n",
    "        #test if self.test = true\n",
    "        if self.episode_count % self.n_train == 0 and self.test:\n",
    "            self.test_games(n_test = self.n_test)\n",
    "            self.opponent.epsilon = 0.5\n",
    "            if print_time and self.M_rand[-1] >= self.M_rand_threshold and self.M_opt[-1] >= self.M_opt_threshold :\n",
    "              end = time.time()\n",
    "              print('Converged in ', end-start)\n",
    "              print_time = False\n",
    "        \n",
    "        if self.episode_count % 2500 == 0:\n",
    "          self.alpha = 0.5*self.alpha\n",
    "          self.optimizer = keras.optimizers.Adam(learning_rate = self.alpha)\n",
    "\n",
    "        if self.episode_count >= self.N :\n",
    "          print(\"Done {} training games\".format(self.N))\n",
    "          return self.M_rand, self.M_opt\n",
    "\n",
    "\n",
    "  def test_games(self, eps_dq_player = 0., n_test = 500):\n",
    "      \"\"\"\n",
    "      Function to play n test games\n",
    "      INPUTS:\n",
    "      - eps_dq_player: exploration level of dq_player\n",
    "      - n_test: number of test games\n",
    "      \"\"\"\n",
    "      rewards_rand = 0\n",
    "      rewards_opt = 0\n",
    "      for i in range(n_test):\n",
    "        self.dq_player1.set_player('O',i)\n",
    "        self.opponent.set_player('X',i)\n",
    "        new_game = self.game_test(eps_dq_player, 1.)\n",
    "        rewards_rand += new_game      \n",
    "        rewards_opt += self.game_test(eps_dq_player, 0.)\n",
    "      rewards_rand = rewards_rand/n_test\n",
    "      rewards_opt = rewards_opt/n_test\n",
    "      self.M_rand.append(rewards_rand)\n",
    "      self.M_opt.append(rewards_opt)\n",
    "\n",
    "  def game_test(self, eps_dq_player = 0., eps_opponent = 0.):\n",
    "        \"\"\"\n",
    "        Function to play a test game\n",
    "        INPUTS:\n",
    "        - eps_q_player: exploration level of q_player\n",
    "        - eps_opponent: exploration level of opponent\n",
    "        \"\"\"\n",
    "        self.env.reset() # reset the board\n",
    "        #setting exploration levels\n",
    "        self.dq_player1.epsilon = eps_dq_player\n",
    "        self.opponent.epsilon = eps_opponent\n",
    "        #print('eps opp test', self.opponent.epsilon)\n",
    "        while not self.env.end: # while the game is not over\n",
    "            \n",
    "          if self.env.current_player == self.dq_player1.player_name :\n",
    "                # action of the q_player\n",
    "                state_tensor = self.build_tensor(self.env.grid.copy(),self.dq_player1.player_name)\n",
    "\n",
    "                if self.dq_player1.epsilon > np.random.rand(1)[0]: action = np.random.choice(self.num_actions)\n",
    "                else:\n",
    "                  # Predict action Q-values\n",
    "                  action_values = self.model_target(state_tensor, training=False)\n",
    "                  # Take best action\n",
    "                  action = tf.argmax(action_values[0]).numpy()\n",
    "                try:\n",
    "                  self.env.step(int(action))\n",
    "                except ValueError:\n",
    "                    self.env.end = True\n",
    "                    self.env.winner = self.opponent.player\n",
    "          else:\n",
    "              # action of the opponent\n",
    "              move_opp = self.opponent.act(self.env.grid)\n",
    "              self.env.step(move_opp)\n",
    "\n",
    "        # return reward of the q-player\n",
    "        return self.env.reward(self.dq_player1.player_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 16\n",
    "np.random.seed(2022)\n",
    "random.seed(2023)\n",
    "eps_vector = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "rew_epsilon = []\n",
    "\n",
    "\n",
    "n_train = 250\n",
    "n_test = 500\n",
    "N = 20000\n",
    "\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "\n",
    "ax1.set_title('M_opt')\n",
    "ax2.set_title('M_rand')\n",
    "\n",
    "for epsilon in eps_vector:\n",
    "    sdql = SelfDQLearning(epsilon*np.ones([N,1]), N = N, n_train = n_train,  test = True, n_test = n_test, alpha = 1e-3)\n",
    "    M_rand_vec, M_opt_vec = sdql.training()\n",
    "    ax1.plot(np.arange(1,len(M_opt_vec)+1)*250, M_opt_vec, label = 'eps='+str(epsilon))\n",
    "    ax2.plot(np.arange(1,len(M_rand_vec)+1)*250, M_rand_vec, label = 'eps='+str(epsilon))\n",
    "\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "\n",
    "plt1.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n",
    "plt2.legend(bbox_to_anchor=(0.94,1), loc=\"upper left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 17\n",
    "random.seed(2022)\n",
    "np.random.seed(2023)\n",
    "n_train = 250\n",
    "n_test = 500\n",
    "e_min = 0.1\n",
    "e_max = 0.8\n",
    "N = 20000\n",
    "N_star_vec = [1,1000, 10000, 20000, 30000,40000]\n",
    "batch_size = 64\n",
    "buffer_size = 10000\n",
    "\n",
    "\n",
    "plt1, ax1 = plt.subplots()\n",
    "plt2, ax2 = plt.subplots()\n",
    "\n",
    "for N_star in N_star_vec:\n",
    "  epsilon = []\n",
    "  for i in np.arange(1,N+1):\n",
    "      epsilon.append(max(e_min,e_max*(1-i/N_star)))\n",
    "  sdqn = SelfDQLearning(epsilon = epsilon, N = N, n_train = n_train, test = True, n_test = n_test, alpha = 1e-3)\n",
    "  M_rand, M_opt = sdqn.training()\n",
    "  print('N_star =' + str(N_star) + ' M_opt mean ' + str(np.mean(M_opt[40:])))\n",
    "  print('N_star =' + str(N_star) + ' M_rand mean ' + str(np.mean(M_rand[40:])))\n",
    "  ax1.plot(np.arange(1,len(M_opt)+1)*250, M_opt, label = 'N*='+str(N_star))\n",
    "  ax1.set_title('M_opt')\n",
    "  plt1.legend(bbox_to_anchor=(0.88,0.8), loc=\"upper left\")\n",
    "  ax1.grid()\n",
    "  ax2.plot(np.arange(1,len(M_rand)+1)*250, M_rand, label = 'N*='+str(N_star))\n",
    "  plt2.legend(bbox_to_anchor=(0.9,0.8), loc=\"upper left\")\n",
    "  ax2.grid()\n",
    "  ax2.set_title('M_rand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question 19\n",
    "import seaborn as sn\n",
    "# first state\n",
    "grid1 = np.array([[0.,1.,1.],[0., -1., -1], [0.,1.,-1]])\n",
    "state1 = sdqn.build_tensor(grid1,'X')\n",
    "out1 = sdqn.model_target(state1)\n",
    "sn.heatmap(out1.numpy().reshape(3,3), center = 0, square = True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second state\n",
    "grid2 = np.array([[0.,1.,0.],[0., 0, -1], [0.,1.,-1]])\n",
    "state2 = sdqn.build_tensor(grid2,'X')\n",
    "out2 = sdqn.model_target(state2)\n",
    "sn.heatmap(out2.numpy().reshape(3,3), center = 0, square = True, linewidths=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third state\n",
    "grid3 = np.array([[1.,0.,0.],[0.,-1,0.],[0.,0.,0.]])\n",
    "state3 = sdqn.build_tensor(grid3,'X')\n",
    "out3 = sdqn.model_target(state3)\n",
    "sn.heatmap(out3.numpy().reshape(3,3), center = 0, square = True, linewidths=.5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4f7527b0034a603a2f6ced09004f8a888be406f0f25f6fb9b2bc8cc71cf9bc4b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
