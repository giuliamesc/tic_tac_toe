{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "427b9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c0c8a027",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 0,\n",
       " 'current_player': 'X'}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = TictactoeEnv()\n",
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "ad980c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPlayer:\n",
    "    \n",
    "    def __init__(self, epsilon, alpha = 0.05, gamma = 0.99, player_name = 'O'):\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.player_name = player_name\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.states_opponent = []\n",
    "        self.actions_opponent = []\n",
    "        self.states_value = dict()\n",
    "        \n",
    "    def set_player(self, player_name = 'X', j=-1):\n",
    "        self.player_name = player_name\n",
    "        if j != -1:\n",
    "            self.player_name = 'X' if j % 2 == 0 else 'O'\n",
    "            \n",
    "    def empty(self, grid):\n",
    "        '''return all empty positions'''\n",
    "        avail = []\n",
    "        for i in range(9):\n",
    "            pos = (int(i/3), i % 3)\n",
    "            if grid[pos] == 0:\n",
    "                avail.append(i)\n",
    "        return avail\n",
    "    \n",
    "    def get_state_key(self, grid):\n",
    "        \"Transforms the grid configuration into a string -- needed for the key of the dictionary\" \n",
    "        key = str(grid.reshape(3 * 3))\n",
    "        return key\n",
    "    \n",
    "    def select_optimal_action(self, grid):\n",
    "        avail_actions = self.empty(grid)\n",
    "        key = self.get_state_key(grid)\n",
    "        if key in self.states_value : \n",
    "            restricted_vector = self.states_value[key][avail_actions]\n",
    "            move = np.argmax(restricted_vector)\n",
    "        else :\n",
    "            move = self.select_random_action(grid)\n",
    "        return move\n",
    "    \n",
    "    def select_random_action(self,grid):\n",
    "        actions = self.empty(grid)\n",
    "        move = random.choice(actions)\n",
    "        return move\n",
    "    \n",
    "    def act(self,grid):\n",
    "        b = np.random.binomial(1,1-self.epsilon)\n",
    "        if b == 1 : return self.select_optimal_action(grid)\n",
    "        else : return self.select_random_action(grid)\n",
    "        \n",
    "class QLearning:\n",
    "    def __init__(self, epsilon1, epsilon2, alpha = 0.05, gamma = 0.99):\n",
    "        self.q_player = QPlayer(epsilon1, alpha, gamma)\n",
    "        self.opponent = OptimalPlayer(epsilon2)\n",
    "        self.env = TictactoeEnv()\n",
    "        \n",
    "    def game(self):\n",
    "        self.env.reset()\n",
    "        if self.env.current_player == 'X' : self.q_player.states.append(np.zeros([3,3]))\n",
    "        else : self.q_player.states_opponent.append(np.zeros([3,3]))\n",
    "        while not self.env.end : \n",
    "            if self.env.current_player == 'O' : \n",
    "                move = self.q_player.act(self.env.grid)\n",
    "                self.env.step(move)\n",
    "                temp = self.env.grid.copy()\n",
    "                self.q_player.states.append(temp)\n",
    "                self.q_player.actions.append(move)\n",
    "\n",
    "            else : \n",
    "                move = self.opponent.act(self.env.grid)\n",
    "                self.env.step(move)\n",
    "                temp = self.env.grid.copy()\n",
    "                self.q_player.states_opponent.append(temp)\n",
    "                self.q_player.actions_opponent.append(3*move[0]+move[1])\n",
    "        self.update_q('O',self.q_player.states,self.q_player.actions)\n",
    "        self.update_q('X',self.q_player.states_opponent,self.q_player.actions_opponent)\n",
    "        \n",
    "        \n",
    "    #def give_reward(self):\n",
    "        \n",
    "    def update_q(self, player, states, actions):\n",
    "        r = self.env.reward(player)\n",
    "        i = len(actions)-1\n",
    "        key = self.q_player.get_state_key(self.env.grid)\n",
    "        self.q_player.states_value[key] = np.zeros([9,1])\n",
    "        \n",
    "        prov = states[::-1]\n",
    "        \n",
    "        for j in range(1,len(prov)):\n",
    "            st = prov[j]\n",
    "            st_prec = prov[j-1]\n",
    "            key_prec = self.q_player.get_state_key(st_prec)\n",
    "            key = self.q_player.get_state_key(st)\n",
    "            print(self.q_player.states_value[key_prec])\n",
    "            if key not in self.q_player.states_value:\n",
    "                self.q_player.states_value[key] = np.zeros([9,1])\n",
    "                \n",
    "            self.q_player.states_value[key][actions[i]] += self.q_player.alpha*(r + self.q_player.gamma*max(self.q_player.states_value[key_prec])-self.q_player.states_value[key][actions[i]])  \n",
    "            r = 0\n",
    "            i -= 1\n",
    "            \n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1e048282",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2022)\n",
    "epsilon = np.random.uniform(0,0.5)\n",
    "game = QLearning(epsilon,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8edc1266",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[ 1. -1. -1.  0.  1.  0.  0.  0.  0.]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/3084815694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/2225965471.py\u001b[0m in \u001b[0;36mgame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_opponent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions_opponent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'O'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_q\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'X'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_opponent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactions_opponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3472/2225965471.py\u001b[0m in \u001b[0;36mupdate_q\u001b[1;34m(self, player, states, actions)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mkey_prec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst_prec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_prec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_player\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '[ 1. -1. -1.  0.  1.  0.  0.  0.  0.]'"
     ]
    }
   ],
   "source": [
    "game.game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5384be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
